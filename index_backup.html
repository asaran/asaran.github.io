<!DOCTYPE HTML>
<html lang="en"><head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
   <script async src="https://www.googletagmanager.com/gtag/js?id=UA-162765518-1"></script>
   <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-162765518-1');
   </script>


  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Akanksha Saran</title>
   
  
  <!--<meta name="author" content="Akanksha Saran">-->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/sony-icon.png"> 

  <style>
    :root {
      --container: 800px; /* smaller container for bio and news*/
      --container-wide: 960px; /* bigger container for research*/
    }

    .container {
      max-width: var(--container);
    }

    .container-wide {
      max-width: var(--container-wide);
    }

    .hidden {
      display: none;
    }
  </style>

    <script type="text/javascript">
      let activeBlock = null
      let $hiddenParaBlock = null
      function toggleblock(selector, descParaSelector) {
        let $descPara = descParaSelector ? document.getElementById(descParaSelector) : null
        if (activeBlock) {
          document.getElementById(activeBlock).style.display = 'none'
        }
        document.getElementById(selector).style.display = selector === activeBlock ? 'none' : 'block'
        if (activeBlock === selector) {
          activeBlock = null
          if ($descPara) {
            $descPara.style.display = 'block'
          }
        } else {
          activeBlock = selector
          if ($hiddenParaBlock && $hiddenParaBlock !== $descPara) {
            $hiddenParaBlock.style.display = 'block'
          }
          if ($descPara) {
            $descPara.style.display = 'none'
            $hiddenParaBlock = $descPara
          }
        }
      }


    </script>

</head>

<body>

  <style>
    .profile-photo-link img {
      width: 80%;
            max-width: 80%;
    }
    @media (max-width: 500px) {
      .about-section {
        display: flex;
        flex-direction: column-reverse;
        align-items: center;
      }

      .profile-photo-link {
        display: block;
        
      }

      .profile-photo-link img {
        width: 100%;
        max-width: 100%;
        display: block;
      }
    }
  </style>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table class="container" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr class="about-section" style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Akanksha Saran</name><br>
		                  akanksha dot saran at sony dot com
              </p>
              <!-- <p> I am a Postdoctoral Researcher in the <a href="https://www.microsoft.com/en-us/research/theme/reinforcement-learning-group/">Reinforcement Learning Group</a> 
                at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-york/">Microsoft Research NYC</a> where I think about research problems 
                related to human-interactive machine learning and sequential decision making. 
                I am intrigued by questions from real-world applications which drive my scientific curiosity. Several of my projects have been tested/deployed on end-user products and applications.
                </p> -->

                <p> I am a Research Scientist in the <a href="https://ai.sony/projects/gaming_ai/">Reinforcement Learning Group</a> 
                  at <a href="https://research.sony">Sony Research</a>. I am interested in research problems 
                  related to human-interactive machine learning and sequential decision making. 
                  I am intrigued by questions from real-world applications which drive my scientific curiosity. 
                  </p>
                
                <!-- <p>I graduated with a PhD degree in Computer Science from <a href="https://www.cs.utexas.edu/">The University of Texas at Austin</a> and a MS in Robotics degree from <a href="https://www.ri.cmu.edu/">Carnegie Mellon University</a>. 
                My PhD dissertation characterized intentions of human teachers via multimodal signals (visual attention and speech) present during demonstrations provided to robots or simulated agents, 
                to inform the design of novel learning from demonstration methods. -->

                <p>Previously, I was a postdoctoral researcher in the <a href="https://www.microsoft.com/en-us/research/theme/reinforcement-learning-group/">Reinforcement Learning Group</a> at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-york/">Microsoft Research New York</a>, prior to which I graduated with a PhD in Computer Science from <a href="https://www.cs.utexas.edu/">The University of Texas at Austin</a> and MS in Robotics from <a href="https://www.ri.cmu.edu/">Carnegie Mellon University</a>. 
                  My PhD dissertation characterized intentions of human teachers via multimodal signals (visual attention and speech) present during demonstrations provided to robots or simulated agents, 
                  to inform the design of novel learning from demonstration methods. 
                  <!-- My postdoctoral research explored theoretical questions and practical applications related to various aspects of interactive machine learning such as learning from observations, active learning, interaction-grounded learning, and learning from implicit human feedback. -->
                </p>

              <p style="text-align:center">
                <a href="mailto:Akanksha.Saran@sony.com">Email</a> &nbsp/&nbsp
                <a href="bio.html">Bio</a> &nbsp/&nbsp
                <!-- <a href="data/cv.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=zZhWSQ0AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/asaran/">GitHub</a>&nbsp/&nbsp
                <a href="https://www.linkedin.com/in/akanksha-saran/">LinkedIn</a> 
                <!-- <a href="https://twitter.com/AkankshaSaran">Twitter</a>  -->
                
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a class="profile-photo-link" href="images/akanksha4.jpg"><img alt="profile photo" src="images/akanksha4.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <hr class="container"/>
        <style>
          .news-content {
            margin: 1em 0;
          }
          .news-ul {
            margin: 0;
          }
          .show-more-news-content-btn {
            margin-top: 0.75rem;
            margin-left: 1.1rem;
            background: none;
            outline: none;
            color: #1772d0;
            border: none;
          }
        </style>
       
        <table class="container" width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr><td>
            <heading>&nbsp;&nbsp;News</heading>
            <div class="news-content">
              <ul class="news-ul"> 
                <li> Our paper <a href="papers/icmi2024.pdf">Prosody as a Teaching Signal for Agent Learning: Exploratory Studies and Algorithmic Implications</a> was accepted to the <a href="https://icmi.acm.org/2024/">ACM International Conference on Multimodal Interaction (ICMI 2024)</a> as an oral presentation.</li>
                <li> Our workshop proposal on <a href="https://rlbrew-workshop.github.io/index.html">Reinforcement Learning Beyond Rewards</a> was accepted to the <a href="https://rl-conference.cc/">1st Reinforcement Learning Conference (RLC 2024)</a>.</li>
                <li> Our paper <a href="https://arxiv.org/pdf/2403.13765.pdf">Towards Principled Representation Learning from Videos for Reinforcement Learning</a> was accepted to <a href="https://iclr.cc/Conferences/2024">ICLR 2024</a> as a spotlight presentation (top 25&#37;).</li>
                <li> I joined the <a href="https://ai.sony/projects/gaming_ai/">reinforcement learning group</a> at <a href="https://research.sony">Sony Research</a> as a Research Scientist.</li>
                <li> Accepted to the <a href="https://eecsrisingstars2023.cc.gatech.edu/participants/">cohort</a> of the <a href="https://eecsrisingstars2023.cc.gatech.edu">2023 EECS Rising Stars Workshop</a>.</li>
                <li> Invited talk at <a href="https://openai.com">OpenAI</a>.</li>
                <li> Co-organized the workshop on <a href="https://interactive-learning-implicit-feedback.github.io">Interactive Learning with Implicit Human Feedback</a> at <a href="https://icml.cc/Conferences/2023/Dates">ICML 2023</a>. You can watch the recording of the event <a href="https://icml.cc/virtual/2023/workshop/21477">here</a>.</li>
                <li> Presented our paper on <a href="https://arxiv.org/abs/2303.02535">Streaming Active Learning with Deep Neural Networks</a> at <a href="https://icml.cc/Conferences/2023/Dates">ICML 2023</a>.</li>
                
              <!-- </div> -->
                </ul>
                <ul id="js-news-show-more-content" class="news-ul hidden">
                <li> Invited talk at <a href="https://www.tri.global">Toyota Research Institute</a>.</li> 
                <li> Invited talk at <a href="https://robotics.cs.washington.edu">University of Washington</a>.</li> 
                <li> Invited talk at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/">Microsoft Research Redmond</a>.</li> 
                <li> <a href="https://www.microsoft.com/en-us/research/blog/inferring-rewards-through-interaction/">MSR</a> and <a href="https://aihub.org/2023/04/04/learning-personalized-reward-functions-with-interaction-grounded-learning-igl/">AI Hub</a> covered our recent ICLR paper <a href="http://arxiv.org/abs/2211.15823">Personalized Reward Learning with Interaction-Grounded Learning</a> as blog posts.</li>
                <li> Our paper on <a href="https://arxiv.org/abs/2303.02535">Streaming Active Learning with Deep Neural Networks</a> was accepted at <a href="https://icml.cc/Conferences/2023/Dates">ICML 2023</a>.</li>
                <li> New <a href="https://www.microsoft.com/en-us/research/blog/unifying-learning-from-preferences-and-demonstration-via-a-ranking-game-for-imitation-learning/?OCID=msr_blog_ImitationLearning_LI">blog post</a> featuring our recent TMLR paper <a href="https://arxiv.org/abs/2202.03481">A Ranking Game for Imitation Learning</a>.</li>
                <li> Invited talk at <a href="https://www.ai.sony">Sony Research</a>.</li> 
                <li> Invited talk at <a href="https://cpsc.yale.edu/research/primary-areas/robotics">Yale University</a>.</li>
                <!-- <li> Invited talk at <a href="https://www.ai.sony">Sony AI Research</a>.</li> 
                <li> Invited talk at <a href="https://cpsc.yale.edu/research/primary-areas/robotics">Yale University</a>.</li>
                <li> Invited talk at <a href="https://www.usc.edu/">USC</a>.</li>  -->
                <!--<li> New <a href="https://aihub.org/2023/04/04/learning-personalized-reward-functions-with-interaction-grounded-learning-igl/">blog post</a> covering our recent ICLR paper <a href="http://arxiv.org/abs/2211.15823">Personalized Reward Learning with Interaction-Grounded Learning</a>.</li> -->
                <!--<li> Our workshop proposal on <a href="https://icml.cc/Conferences/2023/Schedule?showEvent=21477">Interactive Learning with Implicit Human Feedback</a> was accepted at <a href="https://icml.cc/Conferences/2023/CallForWorkshops">ICML 2023</a>!</li> -->
                <li> Invited talk at <a href="https://www.usc.edu/">USC</a>.</li> 
                <li> Our paper <a href="http://arxiv.org/abs/2211.15823">Personalized Reward Learning with Interaction-Grounded Learning</a> was accepted at <a href="https://iclr.cc">ICLR 2023</a>.</li>
                <li> Our paper <a href="https://arxiv.org/abs/2202.03481">A Ranking Game for Imitation Learning</a> was accepted by <a href="https://jmlr.org/tmlr/">TMLR</a>.</li> 
                <li> Co-organized an interdisciplinary workshop at <a href="https://nips.cc">NeurIPS 2022</a> on <a href="https://attention-learning-workshop.github.io">All Things Attention: Bridging Different Perspectives on Attention</a>. Watch the recording of the event <a href="https://nips.cc/virtual/2022/workshop/49996">here</a>.</li>
                <!--<li> Our paper <a href="https://arxiv.org/abs/2202.03481">A Ranking Game for Imitation Learning</a> was accepted at the <a href="https://sites.google.com/view/deep-rl-workshop-neurips-2022">Deep Reinforcement Learning Workshop</a> at <a href="https://nips.cc">NeurIPS 2022</a>.</li> -->
                <li> Our work on <a href="https://arxiv.org/pdf/2206.08364.pdf">Interaction-Grounded Learning with Action-Inclusive Feedback</a> was accepted at <a href="https://nips.cc">NeurIPS 2022</a>.</li>
                <!--<li> Our work on <a href="https://orsum.inesctec.pt/orsum2022/assets/files/paper5.pdf">Interaction-Grounded Learning for Recommender Systems</a> was accepted at the <a href="https://orsum.inesctec.pt/orsum2022/">Workshop on Online Recommender Systems and User Modeling</a>, <a href="https://recsys.acm.org">RecSys 2022</a>.</li> -->
                <li> Our work on <a href="http://arxiv.org/abs/2211.00352">understanding acoustic patterns of human demonstrators</a> was accepted at <a href="https://iros2022.org">IROS 2022</a>. </li>
                <li> Our recent work on <a href="papers/AI-IGL.pdf">Interaction-Grounded Learning with Action-Inclusive Feedback</a> was accepted at the <a href="https://cfol-workshop.github.io">Workshop on Complex Feedback in Online Learning</a> at <a href="https://icml.cc">ICML 2022</a>.</li>
                <li> I moved to New York City and started my postdoc at MSR NYC.</li>
                <!--<li> Invited as a panelist for the <a href="https://twitter.com/iitjodhpur/status/1466732355280801794">2021 Undergraduate Orientation</a> at my alma mater <a href="https://iitj.ac.in/">IIT Jodhpur</a>.</li> -->
                <li> Defended my PhD dissertation on Leveraging Multimodal Human Cues for Enhanced Robot Learning from Demonstration.</li>
                <li> Invited talk at <a href="https://research.google">Google Research</a>.</li> 
                <li> Invited talk at <a href="https://tech.fb.com/ar-vr/">Facebook Reality Labs</a>.</li>
                <li> Invited talk at <a href="https://www.research.ibm.com/labs/watson/">IBM Research</a>.</li>
                <li> Invited talk at <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a>.</li>
                <!--<<li> Presented our work on <a href="https://arxiv.org/abs/2002.12500">Efficiently Guiding Imitation Learning Agents with Human Gaze</a> during the Humans and AI session at <a href="https://aamas2021.soton.ac.uk/">AAMAS 2021</a>.</li>-->
                <!--<li> Invited talk at the <a href="https://sites.google.com/view/realworldhri-workshop/hri21/speakers-schedules?authuser=0">Workshop on Solutions for Socially Intelligent HRI in Real-World Scenarios</a> at <a href="https://humanrobotinteraction.org/2021/">HRI 2021</a>.</li>-->
                <!--<li> I served as a Program Chair for the <a href="http://www.hripioneers.info/hri21/program_speakers.html">HRI Pioneers workshop</a> at <a href="https://humanrobotinteraction.org/2021/">HRI 2021</a>.</li>-->
                </ul>

                <button id="js-show-more-news-content-btn" class="show-more-news-content-btn" type="button">
                  ⇩ Show More
                </button>
            </div>
          </td></tr>
        </table>
        <script type="text/javascript">
          const $btn = document.getElementById('js-show-more-news-content-btn')
          const $newsShowMoreContent = document.getElementById('js-news-show-more-content')
          let isShowMoreNewsContentVisible = false
          $btn.addEventListener('click', function(event) {
            isShowMoreNewsContentVisible = !isShowMoreNewsContentVisible
            if (isShowMoreNewsContentVisible) {
              $btn.textContent = "⇧ Show Less"
              $newsShowMoreContent.classList.remove('hidden')
            } else {
              $newsShowMoreContent.classList.add('hidden')
              $btn.textContent = "⇩ Show More"
            }
          })
          
        </script>

        <hr class="container-wide"/>

        <table class="container" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>	&#945;-&#946; indicates alphabetical author order, * indicates equal contribution</p>
            </td>
          </tr>
        </tbody></table>
        <style>


.research-table {
  overflow: hidden;
}
        /* image style format*/  
        .research-table img {
          display: block;
          max-width: 100%;
          height: auto;
          margin: 0 auto;  /*center align*/
          /* margin-left: auto; */ /*right align*/
        }

        @media (max-width: 500px) {
       .research-table tr {
          display: flex;
          flex-direction: column;
          width: calc(100vw - 16px);
          min-width: 100%;
          flex-grow: 1;
          height: 100%;
          overflow: hidden;
          padding-bottom: 20px;
          margin-bottom: 20px;
          border-bottom: 1px solid #e1e1e1;
         }

         .research-table td {
           width: 98% !important;
           padding: 5px !important;
         }
       }
        </style>
        <table class="research-table container-wide"  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
       
         
        <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <a href="images/prosody_woz.jpg"> <img src="images/prosody_woz.jpg"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="papers/icmi2024.pdf" id="icmi2024">
              <papertitle>Prosody as a Teaching Signal for Agent Learning: Exploratory Studies and Algorithmic Implications.</papertitle></a><br>
              Matilda Knierim, Sahil Jain, Murat Han Aydoğan, Kenneth Mitra, Kush Desai, <b>Akanksha Saran</b><sup>*</sup>, Kim Baraka<sup>*</sup>.
              <br>ICMI 2024 [Oral] 
              </p>
        
              <div class="paper" id="icmi2024">
              <a href="papers/icmi2024.pdf">pdf</a> |
              <a href="javascript:toggleblock('icmi2024-abstract', 'icmi2024-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('icmi2024-bibtex', 'icmi2024-desc')" class="togglebib">bibtex</a> |
              <a href="https://github.com/sahiljain11/audio_rl">code</a>

              <p align="justify"> <i id="icmi2024-abstract" class="hidden"> 
                Agent learning from human interaction often relies on explicit
                signals, but implicit social cues, such as prosody in speech, could
                provide valuable information for more effective learning. This paper 
                advocates for the integration of prosody as a teaching signal
                to enhance agent learning from human teachers. Through two exploratory 
                studies --- one examining voice feedback in an interactive
                reinforcement learning setup and the other analyzing restricted
                audio from human demonstrations in three Atari games --- we demonstrate 
                that prosody carries significant information about task dynamics. 
                Our findings suggest that prosodic features, when coupled
                with explicit feedback, can enhance reinforcement learning outcomes. 
                Moreover, we propose guidelines for prosody-sensitive algorithm design 
                and discuss insights into teaching behavior. Our work underscores 
                the potential of leveraging prosody as an implicit signal for more 
                efficient agent learning, thus advancing human-agent interaction paradigms.</i></p>
        
                <pre xml:space="preserve" id="icmi2024-bibtex" class="hidden">
  @article{knierim2024prosody,
    title={Prosody as a Teaching Signal for Agent Learning: <br>&emsp;&emsp;&emsp;&emsp;Exploratory Studies and Algorithmic Implications},
    author={Knierim, Matilda and Jain, Sahil and Han Aydoğan, Murat<br>&emsp;&emsp;&emsp;&emsp;and Mitra, Kenneth and Kush Desai and Saran, Akanksha and Baraka, Kim},
    journal={ACM International Conference on Multimodal Interaction (ICMI)},
    year={2024}
  }
                </pre>
              </div>
              <p id="icmi2024-desc"> Prosodic cues from human teachers can enhance learning outcomes for reinforcement learning and imitation learning paradigms.</p>
            </td>
          </tr> 
        
        

          <tr>
            
            <td style="padding:20px;width:30%;vertical-align:middle">
              <a href="images/vid_rep.png"> <img src="images/vid_rep.png"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="https://arxiv.org/abs/2403.13765" id="iclr2024">
              <papertitle>Towards Principled Representation Learning from Videos for Reinforcement Learning</papertitle></a><br>
              Dipendra Misra<sup>*</sup>, <b>Akanksha Saran</b><sup>*</sup>, Tengyang Xie, Alex Lamb, John Langford
              <br>Workshop on Reinforcement Learning Beyond Rewards, RLC 2024
              <br>ICLR 2024 [Spotlight: Top 25&#37;] 
              </p>
        
              <div class="paper" id="iclr2024">
              <a href="https://arxiv.org/pdf/2403.13765.pdf">pdf</a> |
              <a href="javascript:toggleblock('iclr2024-abstract', 'iclr2024-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('iclr2024-bibtex', 'iclr2024-desc')" class="togglebib">bibtex</a> |
              <a href="https://drive.google.com/file/d/1PZmkYz7PU8Cig1vvtL3i2aRBzfwLGO0o/view?usp=sharing">poster</a> |
              <a href="https://iclr.cc/virtual/2024/poster/19497">talk video</a> 
              <!-- <a href="https://github.com/microsoft/Intrepid">code</a>   -->
              
              <p align="justify"> <i id="iclr2024-abstract" class="hidden">We study pre-training representations for decision-making using video data, which is abundantly available for tasks such as game agents and software testing. Even though significant empirical advances have been made on this problem, a theoretical understanding remains absent. We initiate the theoretical investigation into principled approaches for representation learning and focus on learning the latent state representations of the underlying MDP using video data. We study two types of settings: one where there is iid noise in the observation, and a more challenging setting where there is also the presence of exogenous noise, which is non-iid noise that is temporally correlated, such as the motion of people or cars in the background. We study three commonly used approaches: autoencoding, temporal contrastive learning, and forward modeling. We prove upper bounds for temporal contrastive and forward modeling in the presence of only iid noise. We show that these approaches can learn the latent state and use it to do efficient downstream RL with polynomial sample complexity. When exogenous noise is also present, we establish a lower bound result showing that learning from video data can be exponentially worse than learning from action-labeled trajectory data. This partially explains why reinforcement learning with video pre-training is hard. We evaluate these representational learning methods in two visual domains, yielding results that are consistent with our theoretical findings.</i></p>
        
<pre xml:space="preserve" id="iclr2024-bibtex" class="hidden">
@inproceedings{misra2024towards,
  title={Towards Principled Representation Learning 
  from Videos for Reinforcement Learning},
  author={Misra, Dipendra and Saran, Akanksha and Xie, Tengyang and Lamb, 
  Alex and Langford, John.},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}
</pre>

              </div>
              <p id="iclr2024-desc">Theoretical analysis and experiments concerning the value reinforcement learning can gain from pretrained representations of unlabeled video data.</p>
            </td>
          </tr>  
        
        <!--  
        <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <a href="images/EyeO.png"> <img src="images/EyeO.png"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="https://arxiv.org/abs/2307.15039" id="eyeo">
              <papertitle>EyeO: Autocalibrating Gaze Output with Gaze Input.</papertitle></a><br>
              <b>Akanksha Saran</b>, Jacob Alber, Cyril Zhang, Ann Paradiso, Danielle Bragg, John Langford.
              <br>arXiv 2023 
              </p>
        
              <div class="paper" id="eyeo">
              <a href="https://arxiv.org/abs/2307.15039.pdf">pdf</a> |
              <a href="javascript:toggleblock('eyeo-abstract', 'eyeo-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('eyeo-bibtex', 'eyeo-desc')" class="togglebib">bibtex</a> 
              
              <p align="justify"> <i id="eyeo-abstract" class="hidden"> EyeO ...</i></p>
        
                <pre xml:space="preserve" id="eyeo-bibtex" class="hidden">
  @article{saran2023eyeo,
    title={EyeO: Autocalibrating Gaze Output with Gaze Input},
    author={Saran, Akanksha and Alber, Jacob and Zhang, Cyril<br>&emsp;&emsp;&emsp;&emsp;and Paradiso, Ann and Bragg, Danielle and Langford, John},
    journal={arXiv preprint arXiv:2307.15039},
    year={2023}
  }
                </pre>
              </div>
              <p id="eyeo-desc"> gaze autocalibration....</p>
            </td>
          </tr> 
        
        -->


          <tr>
            <!--Update the width:30% and width:70% for text and images to the required proportions-->
            <td style="padding:20px;width:30%;vertical-align:middle">
              <a href="images/VeSSAL3.png"> <img src="images/VeSSAL3.png"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="https://arxiv.org/abs/2303.02535" id="vessal">
              <papertitle>Streaming Active Learning with Deep Neural Networks</papertitle></a><br>
              <b>Akanksha Saran</b>, Safoora Yousefi, Akshay Krishnamurthy, John Langford, Jordan T. Ash
              <br>ICML 2023 
              </p>
        
              <div class="paper" id="vessal">
              <a href="https://arxiv.org/pdf/2303.02535.pdf">pdf</a> |
              <a href="javascript:toggleblock('vessal-abstract', 'vessal-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('vessal-bibtex', 'vessal-desc')" class="togglebib">bibtex</a> |
              <a href="https://github.com/asaran/VeSSAL">code</a>  |
              <a href="posters/icml2023_poster.pdf">poster</a> |
              <a href="https://icml.cc/virtual/2023/poster/24291">talk video</a> 
              
              <p align="justify"> <i id="vessal-abstract" class="hidden">Active learning is perhaps most naturally posed as an online learning problem. However, prior active learning approaches with deep neural networks assume offline access to the entire dataset ahead of time. This paper proposes VeSSAL, a new algorithm for batch active learning with deep neural networks in streaming settings, which samples groups of points to query for labels at the moment they are encountered. Our approach trades off between uncertainty and diversity of queried samples to match a desired query rate without requiring any hand-tuned hyperparameters. Altogether, we expand the applicability of deep neural networks to realistic active learning scenarios, such as applications relevant to HCI and large, fractured datasets.</i></p>
        
                <pre xml:space="preserve" id="vessal-bibtex" class="hidden">
@article{saran2023streaming,
  title={Streaming Active Learning with Deep Neural Networks},
  author={Saran, Akanksha and Yousefi, Safoora and Krishnamurthy, Akshay<br>&emsp;&emsp;&emsp;&emsp;and Langford, John and Ash, Jordan T.},
  journal={arXiv preprint arXiv:2303.02535},
  year={2023}
}
                </pre>
              </div>
              <p id="vessal-desc">An approximate volume sampling approach for streaming batch active learning.</p>
            </td>
          </tr> 


          <!--
          <tr>
            <td style="padding:20px;width:30%;vertical-align:top">
              <a href="images/audio_rl.png"> <img src="images/audio_rl.png"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="papers/audio_rl.pdf" id="hrilbr23">
              <papertitle>&ldquo;Yeess!&rdquo; &ldquo;No-oh-oh...&rdquo; Implicit Robot Task Information from Prosody in Human Verbal Feedback</papertitle></a><br>
              Murat Han Aydo&#x11F;an, Kenneth D Mitra, Kush Desai, Taylor Kessler Faulkner, Akanksha Saran<sup>*</sup>, Kim Baraka<sup>*</sup>
              </p>
        
              <div class="paper" id="hrilbr23">
              <a href="papers/audio_rl.pdf">pdf</a> |
              <a href="javascript:toggleblock('hrilbr23-abstract', 'hrilbr23-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('hrilbr23-bibtex', 'hrilbr23-desc')" class="togglebib">bibtex</a> 
              
              <p align="justify"> <i id="hrilbr23-abstract" class="hidden">This paper presents preliminary evidence that prosody carries useful task information in an interactive reinforcement learning setting with verbal evaluative feedback from a human teacher. We developed a novel mixed-participant Wizard-of-Oz study setup to collect audio data from participants teaching a reinforcement learning agent in a grid world navigation task where the agent was wizarded by another participant, hence simulating prosody-sensitive agent learning. Our pilot study shows that, for the participants tested in the teacher role, prosodic features such as energy and pitch are statistically significantly correlated with the advantage function, an underlying Markov Decision Process feature used by RL algorithms. Results also suggest some level of individual differences between different teachers. While further research is needed to develop computational models of human teachers' prosody in different learning tasks, our early results highlight the potential of tapping into implicit voice signals to improve robot learning and human teaching efficiency.</i></p>
        
                <pre xml:space="preserve" id="hrilbr23-bibtex" class="hidden">
@inproceedings{Aydo&#x11F;an2023yeess,
  title={&ldquo;Yeess!&rdquo; &ldquo;No-oh-oh...&rdquo; Implicit Robot Task Information<br>&emsp;&emsp;from Prosody in Human Verbal Feedback},
  author={Aydo&#x11F;an, Murat Han  and Mitra, Kenneth D and Desai, Kush<br>&emsp;&emsp;and Faulkner, Taylor Kessler and Saran, Akanksha and Baraka, Kim},
  journal={},
  year={2023}
}
                </pre>
              </div>
              <p id="hrilbr23-desc">Prosodic cues of human teachers providing speech-based feedback to RL agents are correlated with the advantage function of the underlying MDP for a goal-oriented simulation task.</p>
            </td>
          </tr> 
         -->

         <tr>
          <!--Update the width:30% and width:70% for text and images to the required proportions-->
          <td style="padding:20px;width:30%;vertical-align:top">
            <!-- <a href="images/igl_3states.png"> <img src="images/igl_3states.png"></a></td> -->
            <a href="images/IGL-P(3).png"> <img src="images/IGL-P(3).png"></a></td>
            <td style="padding:20px;width:70%;vertical-align:top">
            <!-- <p><a href="https://orsum.inesctec.pt/orsum2022/assets/files/paper5.pdf" id="orsum22"> -->
            <p><a href="https://arxiv.org/abs/2211.15823" id="iclr23">
            <papertitle>Personalized Reward Learning with Interaction-Grounded Learning</papertitle></a><br>
            <!--(&#945;-&#946;) Jessica Maghakian, Paul Mineiro, Kishan Panaganti, <b>Akanksha Saran</b>, Cheng Tan-->
            (&#945;-&#946;) Jessica Maghakian, Paul Mineiro, Kishan Panaganti, Mark Rucker, <b>Akanksha Saran</b>, Cheng Tan
            <br>Workshop on Online Recommender Systems and User Modeling, RecSys 2022
            <br>ICLR 2023
            </p>
      
            <div class="paper" id="iclr23">
            <!-- <a href="https://orsum.inesctec.pt/orsum2022/assets/files/paper5.pdf">pdf</a> | -->
            <a href="https://arxiv.org/pdf/2211.15823.pdf">pdf</a> |
            <a href="javascript:toggleblock('iclr23-abstract', 'iclr23-desc')">abstract</a> |
            <a shape="rect" href="javascript:toggleblock('iclr23-bibtex', 'iclr23-desc')" class="togglebib">bibtex</a> |
            <a href="https://github.com/asaran/IGL-P">code</a> |
            <a href="posters/iclr2023_poster.pdf">poster</a> |
            <a href="posters/iclr2023_slides.pdf">slides</a> |
            <a href="https://aihub.org/2023/04/04/learning-personalized-reward-functions-with-interaction-grounded-learning-igl/">blog</a> |
            <a href="https://www.microsoft.com/en-us/research/blog/inferring-rewards-through-interaction/">blog</a> |
            <a href="https://iclr.cc/virtual/2023/poster/11002">talk video</a>
            
            
            <p align="justify"> <i id="iclr23-abstract" class="hidden">In an era of countless content offerings, recommender systems alleviate information 
              overload by providing users with personalized content suggestions. Due to the scarcity of explicit user feedback, modern recommender systems 
              typically optimize for the same fixed combination of implicit feedback signals across all users. However, this approach disregards a growing 
              body of work highlighting that (i) implicit signals can be used by users in diverse ways, signaling anything from satisfaction to active dislike, 
              and (ii) different users communicate preferences in different ways. We propose applying the recent Interaction Grounded Learning (IGL) paradigm to 
              address the challenge of learning representations of diverse user communication modalities. Rather than taking a fixed, human-designed reward function, 
              IGL is able to learn personalized reward functions for different users and then optimize directly for the latent user satisfaction. We demonstrate 
              the success of IGL with experiments using simulations as well as with real-world production traces.</i></p>
      
              <pre xml:space="preserve" id="iclr23-bibtex" class="hidden">
@inproceedings{maghakian2023personalized,
  title={Personalized Reward Learning with Interaction-Grounded Learning},
  author={Maghakian, Jessica and Mineiro, Paul and Panaganti, Kishan
  and Rucker, Mark and Saran, Akanksha and Tan, Cheng},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023}
}
              </pre>
            </div>
             <!-- <p id="orsum22-desc">A novel personalized variant of IGL for recommender systems that can leverage explicit and implicit user feedback to maximize user satisfaction, with no feedback signal modeling and minimal assumptions.</p>-->
            <p id="iclr23-desc">A novel personalized variant of IGL: the first IGL strategy for context-dependent feedback, the first use of inverse kinematics as an IGL objective, and the first IGL strategy for more than two latent states.</p>
          </td>
        </tr> 



         <tr>
            <td style="padding:20px;width:30%;vertical-align:top">
              <a href="images/rank-game-pref-labels.png">  <img src="images/rank-game-pref-labels.png"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="https://arxiv.org/abs/2202.03481" id="rank-game-arxiv22">
              <papertitle>A Ranking Game for Imitation Learning</papertitle></a><br>
              Harshit Sikchi, <b>Akanksha Saran</b>, Wonjoon Goo, Scott Niekum
              <br>Deep Reinforcement Learning Workshop, NeurIPS 2022
              <br>TMLR 2023
              </p>
        
              <div class="paper" id="rank-game-arxiv22">
              <a href="https://arxiv.org/pdf/2202.03481.pdf">pdf</a> |
              <a href="javascript:toggleblock('rank-game-arxiv22-abstract', 'rank-game-arxiv22-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('rank-game-arxiv22-bibtex', 'rank-game-arxiv22-desc')" class="togglebib">bibtex</a> |
              <a href="https://github.com/hari-sikchi/rank-game">code</a> |
              <a href="https://hari-sikchi.github.io/rank-game/">project webpage</a> |
              <a href="https://www.youtube.com/watch?v=gTf8WoYUOH8">talk video</a> |
              <a href="https://www.microsoft.com/en-us/research/blog/unifying-learning-from-preferences-and-demonstration-via-a-ranking-game-for-imitation-learning/?OCID=msr_blog_ImitationLearning_LI">blog</a>
              
              
              <p align="justify"> <i id="rank-game-arxiv22-abstract" class="hidden">We propose a new framework for imitation learning — treating imitation as a two-player rankingbased Stackelberg game between a policy and a
                reward function. In this game, the reward agent
                learns to satisfy pairwise performance rankings
                within a set of policies, while the policy agent
                learns to maximize this reward. This game encompasses a large subset of both inverse reinforcement learning (IRL) methods and methods
                which learn from offline preferences. The Stackelberg game formulation allows us to use optimization methods that take the game structure into
                account, leading to more sample efficient and stable learning dynamics compared to existing IRL
                methods. We theoretically analyze the requirements of the loss function used for ranking policy
                performances to facilitate near-optimal imitation
                learning at equilibrium. We use insights from this
                analysis to further increase sample efficiency of
                the ranking game by using automatically generated rankings or with offline annotated rankings.
                Our experiments show that the proposed method
                achieves state-of-the-art sample efficiency and is
                able to solve previously unsolvable tasks in the
                Learning from Observation (LfO) setting. </i></p>
        
                <pre xml:space="preserve" id="rank-game-arxiv22-bibtex" class="hidden">
@inproceedings{sikchi2022ranking,
  title={A Ranking Game for Imitation Learning},
  author={Sikchi, Harshit and Saran, Akanksha and Goo,<br>&emsp;&emsp;Wonjoon and Niekum, Scott},
  booktitle={Transactions on Machine Learning Research (TMLR)},
  year={2023}
}
                </pre>
              </div>
              <p id="rank-game-arxiv22-desc">Treating imitation learning as a two-player ranking game between a policy and a reward function can solve previously unsolvable tasks in the Learning from Observation (LfO) setting.</p>
            </td>
          </tr>   

          

          

          <tr>
            <!--Update the width:30% and width:70% for text and images to the required proportions-->
            <td style="padding:20px;width:10%;vertical-align:top">
              <a href="images/ai-igl.png">  <img src="images/ai-igl.png"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="https://arxiv.org/abs/2206.08364" id="ai-igl-neurips22">
              <papertitle>Interaction-Grounded Learning with Action-Inclusive Feedback</papertitle></a><br>
              Tengyang Xie<sup>*</sup>, <b>Akanksha Saran</b><sup>*</sup>, Dylan Foster, Lekan Molu, Ida Momennejad, Nan Jiang, Paul Mineiro, John Langford
              <br>Workshop on Complex Feedback for Online Learning, ICML 2022
              <br>NeurIPS 2022
              </p>
        
              <div class="paper" id="ai-igl-neurips22">
              <a href="https://arxiv.org/pdf/2206.08364.pdf">pdf</a> |
              <a href="javascript:toggleblock('ai-igl-neurips22-abstract', 'ai-igl-neurips22-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('ai-igl-neurips22-bibtex', 'ai-igl-neurips22-desc')" class="togglebib">bibtex</a> |
              <a href="https://github.com/asaran/AI-IGL">code</a> |
              <a href="posters/ai_igl_poster.png">poster</a> |
              <a href="https://slideslive.com/38985912">talk video</a> 
              
              <p align="justify"> <i id="ai-igl-neurips22-abstract" class="hidden">Consider the problem setting of Interaction-Grounded Learning 
                (IGL), in which a learner's goal is to optimally interact with the environment with no explicit reward to ground its policies. 
                The agent observes a context vector, takes an action, and receives a feedback vector, using this information to effectively 
                optimize a policy with respect to a latent reward function. Prior analyzed approaches fail when the feedback vector contains 
                the action, which significantly limits IGL&#39;s success in many potential scenarios such as Brain-computer interface (BCI) 
                or Human-computer interface (HCI) applications. We address this by creating an algorithm and analysis which allows IGL to 
                work even when the feedback vector contains the action, encoded in any fashion. We provide theoretical guarantees and 
                large-scale experiments based on supervised datasets to demonstrate the effectiveness of the new approach. </i></p>
        
                <pre xml:space="preserve" id="ai-igl-neurips22-bibtex" class="hidden">
@inproceedings{xie2022interaction,
  title={Interaction Grounded Learning with Action-Inclusive Feedback},
  author={Xie, Tengyang and Saran, Akanksha and Foster, Dylan and <br>&emsp;&emsp;Molu, Lekan and Momennejad, Ida and Jiang, Nan <br>&emsp;&emsp;and Mineiro, Paul and Langford, John},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}
                </pre>
              </div>
              <p id="ai-igl-neurips22-desc">An algorithm (AI-IGL) that learns to interpret signals from a controller in an interactive loop without any formal calibration of signal to control
              --- leveraging implicit feedback which can include the action information, but no explicit rewards are available.</p>
            </td>
          </tr> 

          
          <tr>
            <td style="padding:20px;width:30%;vertical-align:top">
              <a href="images/audio.png"> <img src="images/audio.png"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="http://arxiv.org/abs/2211.00352" id="IROS22">
              <papertitle>Understanding Acoustic Patterns of Human Teachers Demonstrating Manipulation Tasks to Robots</papertitle></a><br>
              <b>Akanksha Saran<sup>*</sup></b>, Kush Desai<sup>*</sup>, Mai Lee Chang, Rudolf Lioutikov, Andrea Thomaz, Scott Niekum
              <br>IROS 2022
              </p>
        
              <div class="paper" id="iros22">
              <a href="https://arxiv.org/pdf/2211.00352.pdf">pdf</a> |
              <a href="javascript:toggleblock('iros22-abstract', 'iros22-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('iros22-bibtex', 'iros22-desc')" class="togglebib">bibtex</a> |
              <a href="images/audio_understand.png">spotlight</a> 
              
              <p align="justify"> <i id="iros22-abstract" class="hidden">Humans use audio signals in the form of spoken language or verbal reactions effectively when teaching new skills or tasks to other humans. While demonstrations allow humans to teach robots in a natural way,
                learning from trajectories alone does not leverage other available modalities including audio from human teachers.
                To effectively utilize audio cues accompanying human demonstrations, first it is important to understand what kind of information is present and conveyed by such cues. 
                This work characterizes audio from human teachers demonstrating multi-step manipulation tasks to a situated Sawyer robot using three feature types: (1) duration of speech used, (2) expressiveness in speech or prosody, and (3) semantic content of speech. 
                We analyze these features along four dimensions and find that teachers convey similar semantic concepts via spoken words for different conditions of (1) demonstration types, (2) audio usage instructions, (3) subtasks, and (4) errors during demonstrations. 
                However, differentiating properties of speech in terms of duration and expressiveness are present along the four dimensions, highlighting that human audio carries rich information, potentially beneficial for technological advancement of robot learning from demonstration methods.</i></p>
        
                <pre xml:space="preserve" id="iros22-bibtex" class="hidden">
@inproceedings{saran2022understanding,
  title={Understanding acoustic patterns of human teachers<br>&emsp;&emsp;demonstrating manipulation tasks to robots},
  author={Saran, Akanksha and Desai, Kush and Chang, Mai Lee and<br>&emsp;&emsp;Lioutikov, Rudolf and Thomaz, Andrea and Niekum, Scott},
  booktitle={2022 IEEE/RSJ International Conference on<br>&emsp;&emsp;Intelligent Robots and Systems (IROS)},
  year={2022},
  organization={IEEE}
}
                </pre>
              </div>
              <!-- <p>Human teachers demonstrating goal-oriented manipulation tasks to robots fixate more of their eye gaze movements on objects important for the manipulation in comparison to other objects in the scene. Incorporating this finding during subtask classification and bayesian inverse reinforcement learning improves performance of these learning algorithms.</p> -->
              <p id="iros22-desc">Audio cues of human demonstrators carry rich information about subtasks and errors of multi-step manipulation tasks.</p>
            </td>
          </tr> 
          
         <!--
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <a href="images/aux_airl_2.png">  <img src="images/aux_airl_2.png"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="papers/aux_airl.pdf" id="icmlw21">
                <papertitle>Aux-AIRL: End-to-End Self-Supervised Reward Learning for Extrapolating beyond Suboptimal Demonstrations</papertitle></a><br>
                Yuchen Cui, Bo Liu, <b>Akanksha Saran</b>, Stephen Giguere, Peter Stone, Scott Niekum<br>
                Workshop on Self-Supervised Learning for Reasoning <br>and Perception, ICML 2021
              </p>
        
              <div class="paper" id="icmlw21">
              <a href="papers/aux_airl.pdf">pdf</a> |
              <a href="javascript:toggleblock('icmlw21-abstract', 'icmlw21-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('icmlw21-bibtex', 'icmlw21-desc')" class="togglebib">bibtex</a> |
              <a href="https://drive.google.com/file/d/1mzCN9X3g4zB6icu_TOc8qKxZHwNTvvEx/view?usp=sharing">poster</a>
              
              <p align="justify"> <i id="icmlw21-abstract" class="hidden">Real-world human demonstrations are often suboptimal. How to extrapolate beyond suboptimal
                demonstration is an important open research question. We analyze the success
                of a previous state-of-the-art self-supervised reward learning method that requires four sequential
                optimization steps, and propose a simple end-toend imitation learning method Aux-ARIL that extrapolates from suboptimal demonstrations without requiring multiple optimization steps. </i></p>
        
                <pre xml:space="preserve" id="icmlw21-bibtex" class="hidden">
@inproceedings{cui2021airl,
  title={Aux-AIRL: End-to-End Self-Supervised Reward Learning<br>&emsp;&emsp;for Extrapolating beyond Suboptimal Demonstrations},
  author={Cui, Yuchen and Liu, Bo and Saran, Akanksha and <br>&emsp;&emsp;Giguere, Stephen and Stone, Peter and Niekum, Scott},
  booktitle={Proceedings of the ICML Workshop on <br>&emsp;&emsp;Self-Supervised Learning for Reasoning and Perception},
  year={2021}
}
                </pre>
              </div>
              <p id="icmlw21-desc">An end-to-end self-supervised reward learning method that extrapolates beyond suboptimal demonstrations.</p>
            </td>
          </tr>   
        -->

         

          <tr>
            <td style="padding:20px;width:30%;vertical-align:top">
           <a href="images/cgl2.png"> <img src="images/cgl2.png"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="https://arxiv.org/abs/2002.12500" id="AAMAS21">
              <papertitle>Efficiently Guiding Imitation Learning Agents with Human Gaze</papertitle></a><br>
              <b>Akanksha Saran</b>, Ruohan Zhang, Elaine Schaertl Short, Scott Niekum
              <br>Workshop on Reinforcement Learning in Games, AAAI 2020
              <br>AAMAS 2021
              </p>
        
              <div class="paper" id="aamas21">
              <a href="https://arxiv.org/pdf/2002.12500.pdf">pdf</a> |
              <a href="javascript:toggleblock('aamas21-abstract', 'aamas21-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('aamas21-bibtex', 'aamas21-desc')" class="togglebib">bibtex</a> |
              <a href="https://github.com/asaran/IL-CGL">code</a> |
              <a href="https://drive.google.com/file/d/1bvfnkqFGF8O39yaF1QOcvhrOx2CpZyxX/view?usp=sharing">slides</a> |
              <a href="https://drive.google.com/file/d/1tMnPpqOTG7Jxy34ARQVjAJtSArqLTRw6/view?usp=sharing">spotlight</a> |
              <!-- <a href="https://drive.google.com/file/d/1tq07PhWvUAN1-GyGlPh-j9QBGT_gg2tr/view?usp=sharing">demo video</a> | -->
              <a href="https://techxplore.com/news/2020-03-imitation-algorithms-human.html">blog</a>
             
              <p align="justify"> <i id="aamas21-abstract" class="hidden">Human gaze is known to be an intention-revealing signal in human demonstrations of tasks. In this work, we use gaze cues from human demonstrators to enhance the performance of agents trained via three popular imitation learning methods -- behavioral cloning (BC), behavioral cloning from observation (BCO), and Trajectory-ranked Reward EXtrapolation (T-REX). Based on similarities between the attention of reinforcement learning agents and human gaze, we propose a novel approach for utilizing gaze data in a computationally efficient manner, as part of an auxiliary loss function, which guides a network to have higher activations in image regions where the human's gaze fixated. This work is a step towards augmenting any existing convolutional imitation learning agent's training with auxiliary gaze data. Our auxiliary coverage-based gaze loss (CGL) guides learning toward a better reward function or policy, without adding any additional learnable parameters and without requiring gaze data at test time. We find that our proposed approach improves the performance by 95% for BC, 343% for BCO, and 390% for T-REX, averaged over 20 different Atari games. We also find that compared to a prior state-of-the-art imitation learning method assisted by human gaze (AGIL), our method achieves better performance, and is more efficient in terms of learning with fewer demonstrations. We further interpret trained CGL agents with a saliency map visualization method to explain their performance. At last, we show that CGL can help alleviate a well-known causal confusion problem in imitation learning.</i></p>
        
                <pre xml:space="preserve" id="aamas21-bibtex" class="hidden">
@inproceedings{saran2021efficiently,
  title={Efficiently Guiding Imitation Learning Agents with Human Gaze},
  author={Saran, Akanksha and Zhang, Ruohan and Short, Elaine Schaertl<br>&emsp;&emsp;and Niekum, Scott},
  booktitle={International Conference on Autonomous Agents and Multiagent<br>&emsp;&emsp;Systems (AAMAS)},
  year={2021}
}
                </pre>
              </div>
              <!-- <p>Human demonstrators' gaze used as a cue to inform which visual elements of the scene are important for decision-making.</p> -->
              <p id="aamas21-desc">Human demonstrators' overt visual attention can be used as a supervisory signal to guide imitation learning agents during training, such that they <i>at least</i> attend to visual features considered important by the demonstrator.</p>
            </td>
          </tr> 




          <tr>
            <td style="padding:20px;width:30%;vertical-align:top">
              <a href="images/survey2.png">  <img src="images/survey2.png"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="https://www.ijcai.org/Proceedings/2020/0689.pdf" id="IJCAI20">
              <papertitle>Human Gaze Assisted Artificial Intelligence: A Review</papertitle></a><br>
              Ruohan Zhang, <b>Akanksha Saran</b>, Bo Liu, Yifeng Zhu, Sihang Guo, Scott Niekum, Dana H. Ballard, <br>Mary M. Hayhoe<br>
              IJCAI 2020
              </p>
        
              <div class="paper" id="ijcai20">
              <a href="https://www.ijcai.org/Proceedings/2020/0689.pdf">pdf</a> |
              <a href="javascript:toggleblock('ijcai20-abstract', 'ijcai20-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('ijcai20-bibtex', 'ijcai20-desc')" class="togglebib">bibtex</a> 
              
              <p align="justify"> <i id="ijcai20-abstract" class="hidden">Human gaze reveals a wealth of information about
                internal cognitive state. Thus, gaze-related research
                has significantly increased in computer vision, natural language processing, decision learning, and
                robotics in recent years. We provide a high-level
                overview of the research efforts in these fields, including collecting human gaze data sets, modeling gaze behaviors, and utilizing gaze information
                in various applications, with the goal of enhancing communication between these research areas.
                We discuss future challenges and potential applications that work towards a common goal of humancentered artificial intelligence.</i></p>
        
                <pre xml:space="preserve" id="ijcai20-bibtex" class="hidden">
@inproceedings{zhang2020human,
  title={Human gaze assisted artificial intelligence: A review},
  author={Zhang, Ruohan and Saran, Akanksha and Liu, Bo and Zhu, <br>&emsp;&emsp;Yifeng and Guo, Sihang and Niekum, Scott and Ballard,<br>&emsp;&emsp;Dana and Hayhoe, Mary},
  booktitle={IJCAI: Proceedings of the Conference},
  volume={2020},
  pages={4951},
  year={2020},
  organization={NIH Public Access}
}
                </pre>
              </div>
              <p id="ijcai20-desc">A survey paper summarizing gaze-related research in computer vision, natural language processing, decision learning, and
                robotics in recent years.</p>
            </td>
          </tr> 




          <tr>
            <td style="padding:20px;width:30%;vertical-align:top">
              <a href="images/gaze2a.png"> <img src="images/gaze2a.png"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="https://arxiv.org/abs/1907.07202v4" id="CoRL19">
              <papertitle>Understanding Teacher Gaze Patterns for Robot Learning</papertitle></a><br>
              <b>Akanksha Saran</b>, Elaine Schaertl Short, Andrea Thomaz, Scott Niekum
              <br>Short version: HRI Pioneers 2019
              <br>Full version: CoRL 2019
              </p>
        
              <div class="paper" id="corl19">
              <a href="https://arxiv.org/pdf/1907.07202v4.pdf">pdf</a> |
              <a href="javascript:toggleblock('corl19-abstract', 'corl19-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('corl19-bibtex', 'corl19-desc')" class="togglebib">bibtex</a> |
              <a href="https://github.com/asaran/gaze-LfD">code</a> |
              <a href="https://docs.google.com/presentation/d/1UG1iaLKOIfBkvDsjFbHnpo-gOR6LeFdW/edit?usp=sharing&ouid=108019005608041300274&rtpof=true&sd=true">slides</a> |
              <a href="posters/corl2019_poster.pdf">poster</a> |
              <a href="https://youtu.be/b7StSnt85S4?t=12240">talk video</a> |
              <a href="https://www.youtube.com/watch?v=FiCJRD-YluQ">demo video</a>
              
              <p align="justify"> <i id="corl19-abstract" class="hidden">Human gaze is known to be a strong indicator of underlying human intentions and goals during manipulation tasks. This work studies gaze patterns of human teachers demonstrating tasks to robots and proposes ways in which such patterns can be used to enhance robot learning. Using both kinesthetic teaching and video demonstrations, we identify novel intention-revealing gaze behaviors during teaching. These prove to be informative in a variety of problems ranging from reference frame inference to segmentation of multi-step tasks. Based on our findings, we propose two proof-of-concept algorithms which show that gaze data can enhance subtask classification for a multi-step task up to 6% and reward inference and policy learning for a single-step task up to 67%. Our findings provide a foundation for a model of natural human gaze in robot learning from demonstration settings and present open problems for utilizing human gaze to enhance robot learning.</i></p>
        
                <pre xml:space="preserve" id="corl19-bibtex" class="hidden">
@inproceedings{saran2020understanding,
  title={Understanding teacher gaze patterns for robot learning},
  author={Saran, Akanksha and Short, Elaine Schaertl and<br>&emsp;&emsp;Thomaz, Andrea and Niekum, Scott},
  booktitle={Conference on Robot Learning},
  pages={1247--1258},
  year={2020},
  organization={PMLR}
}
                </pre>
              </div>
              <!-- <p>Human teachers demonstrating goal-oriented manipulation tasks to robots fixate more of their eye gaze movements on objects important for the manipulation in comparison to other objects in the scene. Incorporating this finding during subtask classification and bayesian inverse reinforcement learning improves performance of these learning algorithms.</p> -->
              <p id="corl19-desc">Incorporating eye gaze information of human teachers demonstrating goal-oriented manipulation tasks to robots improves perfomance of subtask classification and bayesian inverse reinforcement learning.</p>
            </td>
          </tr> 




          <tr>
            <td style="padding:20px;width:30%;vertical-align:top"> <!--Change to vertical-align:middle -->
              <a href="images/gaze_follow.png"> <img src="images/gaze_follow.png"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="papers/iros2018.pdf" id="ICRA21">
              <papertitle>Human Gaze Following for Human-Robot Interaction</papertitle></a><br>
              <b>Akanksha Saran</b>, Srinjoy Majumdar, Elaine Schaertl Short, Andrea Thomaz, Scott Niekum
              <br>Workshop on Social Robots in the Wild, HRI 2018
              <br>IROS 2018
              </p>
        
              <div class="paper" id="icra21">
              <a href="papers/iros2018.pdf">pdf</a> |
              <a href="javascript:toggleblock('iros18-abstract', 'iros18-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('iros18-bibtex', 'iros18-desc')" class="togglebib">bibtex</a> |
              <a href="https://github.com/Pearl-UTexas/gaze_tracker">code</a> |
              <a href="https://drive.google.com/file/d/1U6cEqJhFFj5Yq31o8LfOeibX-N8TRQug/view">talk video</a> |
              <a href="https://www.youtube.com/watch?v=zTZSpEcpW_A">demo video</a>
              
              <p align="justify"> <i id="iros18-abstract" class="hidden">Gaze provides subtle informative cues to aid fluent
                interactions among people. Incorporating human gaze predictions can signify how engaged a person is while interacting 
                with a robot and allow the robot to predict a human's intentions or goals. We propose a novel approach to predict 
                human gaze fixations relevant for human-robot interaction tasks—-
                both referential and mutual gaze—in real time on a robot. We use a deep learning approach which tracks a human's
                gaze from a robot's perspective in real time. The approach builds on prior work which uses a deep network to predict the 
                referential gaze of a person from a single 2D image. Our work uses an interpretable part of the network, a gaze heat map, 
                and incorporates contextual task knowledge such as location of
                relevant objects, to predict referential gaze. We find that the gaze heat map statistics also capture differences 
                between mutual and referential gaze conditions, which we use to predict whether
                a person is facing the robot's camera or not. We highlight the challenges of following a person's gaze on a 
                robot in real time and show improved performance for referential gaze and mutual gaze prediction.</i></p>
                
                  <pre class="hidden" id="iros18-bibtex" xml:space="preserve">
@inproceedings{saran2018human,
  title={Human gaze following for human-robot interaction},
  author={Saran, Akanksha and Majumdar, Srinjoy and<br>&emsp;&emsp;Short, Elaine Schaertl and Thomaz, Andrea and Niekum, Scott},
  booktitle={2018 IEEE/RSJ International Conference on<br>&emsp;&emsp;Intelligent Robots and Systems (IROS)},
  pages={8615--8621},
  year={2018},
  organization={IEEE}
}
                  </pre>
              </div>
              <p id="iros18-desc">An approach to predict human gaze fixations relevant for human-robot interaction tasks in real time from a robot's 2D camera view.</p>
            </td>
          </tr> 




          <tr>
            <td style="padding:20px;width:30%;vertical-align:top">
              <a href="images/viewpoint2.png"> <img src="images/viewpoint2.png"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="papers/iros2017.pdf" id="IROS17">
              <papertitle>Viewpoint Selection for Visual Failure Detection</papertitle></a><br>
              <b>Akanksha Saran</b>, Branka Lakic, Srinjoy Majumdar, Juergen Hess, Scott Niekum
              <br>IROS 2017
              </p>
        
              <div class="paper" id="iros17">
              <a href="papers/iros2017.pdf">pdf</a> |
              <a href="javascript:toggleblock('iros17-abstract', 'iros17-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('iros17-bibtex', 'iros17-desc')" class="togglebib">bibtex</a> |
              <a href="https://drive.google.com/file/d/1xVbWqnp0fQGa6W7Jc8xPYReAZ2RTQV8P/view?usp=sharing">slides</a> |
              <a href="https://drive.google.com/file/d/1ORHby7ivWtQdoI2lFLyocvLfFu-MMcHa/view?usp=sharing">spotlight</a>
              
              <p align="justify"> <i id="iros17-abstract" class="hidden">The visual difference between outcomes in many
                robotics tasks is often subtle, such as the tip of a screw
                being near a hole versus in the hole. Furthermore, these small
                differences are often only observable from certain viewpoints or
                may even require information from multiple viewpoints to fully
                verify. We introduce and compare three approaches to selecting
                viewpoints for verifying successful execution of tasks: (1) a
                random forest-based method that discovers highly informative
                fine-grained visual features, (2) SVM models trained on features
                extracted from pre-trained convolutional neural networks, and
                (3) an active, hybrid approach that uses the above methods for
                two-stage multi-viewpoint classification. These approaches are
                experimentally validated on an IKEA furniture assembly task
                and a quadrotor surveillance domain.</i></p>
        
                <pre xml:space="preserve" id="iros17-bibtex" class="hidden">
@inproceedings{saran2017viewpoint,
  title={Viewpoint selection for visual failure detection},
  author={Saran, Akanksha and Lakic, Branka and Majumdar, Srinjoy<br>&emsp;&emsp;and Hess, Juergen and Niekum, Scott},
  booktitle={2017 IEEE/RSJ International Conference on<br>&emsp;&emsp;Intelligent Robots and Systems (IROS)},
  pages={5437--5444},
  year={2017},
  organization={IEEE}
}
                </pre>
              </div>
              <p id="iros17-desc">An approach to select a viewpoint (from a set of fixed viewpoints) to visually verify fine-grained task outcomes post robot task executions.</p>
            </td>
          </tr> 

         
          <tr>
            <td style="padding:20px;width:30%;vertical-align:top">
              <a href="images/hands.jpeg">  <img src="images/hands.jpeg"></a></td>
              <td style="padding:20px;width:70%;vertical-align:top">
              <p><a href="papers/iros2015.pdf" id="IROS15">
              <papertitle>Hand Parsing for Fine-Grained Recognition of Human Grasps in Monocular Images</papertitle></a><br>
              <b>Akanksha Saran</b>, Damien Teney, Kris Kitani
              <br>IROS 2015
              </p>
        
              <div class="paper" id="iros15">
              <a href="papers/iros2015.pdf">pdf</a> |
              <a href="javascript:toggleblock('iros15-abstract', 'iros15-desc')">abstract</a> |
              <a shape="rect" href="javascript:toggleblock('iros15-bibtex', 'iros15-desc')" class="togglebib">bibtex</a> 
              
              <p align="justify"> <i id="iros15-abstract" class="hidden">We propose a novel method for performing fine-grained recognition of human hand grasp types using a single
                monocular image to allow computational systems to better
                understand human hand use. In particular, we focus on
                recognizing challenging grasp categories which differ only by
                subtle variations in finger configurations. While much of the
                prior work on understanding human hand grasps has been
                based on manual detection of grasps in video, this is the first
                work to automate the analysis process for fine-grained grasp
                classification. Instead of attempting to utilize a parametric
                model of the hand, we propose a hand parsing framework
                which leverages a data-driven learning to generate a pixelwise segmentation of a hand into finger and palm regions. The
                proposed approach makes use of appearance-based cues such
                as finger texture and hand shape to accurately determine hand
                parts. We then build on the hand parsing result to compute
                high-level grasp features to learn a supervised fine-grained
                grasp classifier. To validate our approach, we introduce a grasp
                dataset recorded with a wearable camera, where the hand
                and its parts have been manually segmented with pixel-wise
                accuracy. Our results show that our proposed automatic hand
                parsing technique can improve grasp classification accuracy
                by over 30 percentage points over a state-of-the-art grasp
                recognition technique. </i></p>
        
                <pre xml:space="preserve" id="iros15-bibtex" class="hidden">
@inproceedings{saran2015hand,
  title={Hand parsing for fine-grained recognition of human grasps<br>&emsp;&emsp;in monocular images},
  author={Saran, Akanksha and Teney, Damien and Kitani, Kris M},
  booktitle={2015 IEEE/RSJ International Conference on<br>&emsp;&emsp;Intelligent Robots and Systems (IROS)},
  pages={5052--5058},
  year={2015},
  organization={IEEE}
}
                </pre>
              </div>
              <p id="iros15-desc">A data-driven approach for fine-grained grasp classification.</p>
            </td>
          </tr>   


        </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Modified version of template from <a style="font-size:small;" href="https://jonbarron.info/">this</a> and <a href="https://www.cs.cmu.edu/~dpathak/">this</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
